{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Extract_Feature","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2zofw3W7EH8I","executionInfo":{"status":"ok","timestamp":1643358487157,"user_tz":-420,"elapsed":26276,"user":{"displayName":"Ngọc Phan Tiến","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01365114518883932849"}},"outputId":"7174854e-43e6-4842-ec0b-ea374c54c63e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n","\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.preprocessing import image\n","\n","from PIL import Image\n","import math\n","import numpy as np\n","import os\n","import json\n","from sklearn.model_selection import train_test_split\n","import cv2"],"metadata":{"id":"_yQACztkX1gp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch \n","import torchvision.models as models\n","import torch\n","import torch.nn as nn\n","import torch.utils.model_zoo as model_zoo\n","\n","import torchvision"],"metadata":{"id":"rtwTUYkuUSpl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##l2-normalization"],"metadata":{"id":"gF82sxjjCSZZ"}},{"cell_type":"code","source":["def l2n(x, eps=1e-6):\n","    return x / (torch.norm(x, p=2, dim=1, keepdim=True) + eps).expand_as(x)"],"metadata":{"id":"kKCB81XEqaSs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class L2N(nn.Module):\n","\n","    def __init__(self, eps=1e-6):\n","        super(L2N,self).__init__()\n","        self.eps = eps\n","\n","    def forward(self, x):\n","        return l2n(x, eps=self.eps)\n","        \n","    def __repr__(self):\n","        return self.__class__.__name__ + '(' + 'eps=' + str(self.eps) + ')'"],"metadata":{"id":"t7SaGVyLqUC8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##GeM"],"metadata":{"id":"sCL8AiibCWUx"}},{"cell_type":"code","source":["import torch.nn.functional as F\n","\n","def gem(x, p=3, eps=1e-6):\n","    return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1./p)"],"metadata":{"id":"BlRATlkDxuw7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.nn.parameter import Parameter\n","\n","class GeM(nn.Module):\n","\n","    def __init__(self, p=3, eps=1e-6):\n","        super(GeM,self).__init__()\n","        self.p = Parameter(torch.ones(1)*p)\n","        self.eps = eps\n","\n","    def forward(self, x):\n","        return gem(x, p=self.p, eps=self.eps)\n","        \n","    def __repr__(self):\n","        return self.__class__.__name__ + '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + ', ' + 'eps=' + str(self.eps) + ')'"],"metadata":{"id":"kmSGjNoLoRwn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Kiến trúc của mạng VGG16 có chỉnh sửa"],"metadata":{"id":"KfkmJBp_Cfba"}},{"cell_type":"code","source":["import torch.nn as nn\n","\n","class ImageRetrievalNet(nn.Module):\n","    \n","    def __init__(self, features, lwhiten, pool, whiten, meta):\n","        super(ImageRetrievalNet, self).__init__()\n","        self.features = nn.Sequential(*features)\n","        self.lwhiten = lwhiten\n","        self.pool = pool\n","        self.whiten = whiten\n","        self.norm = L2N()\n","        self.meta = meta\n","    \n","    def forward(self, x):\n","        # x -> features\n","        o = self.features(x)\n","\n","        # TODO: properly test (with pre-l2norm and/or post-l2norm)\n","        # if lwhiten exist: features -> local whiten\n","        if self.lwhiten is not None:\n","            # o = self.norm(o)\n","            s = o.size()\n","            o = o.permute(0,2,3,1).contiguous().view(-1, s[1])\n","            o = self.lwhiten(o)\n","            o = o.view(s[0],s[2],s[3],self.lwhiten.out_features).permute(0,3,1,2)\n","            # o = self.norm(o)\n","\n","        # features -> pool -> norm\n","        o = self.norm(self.pool(o)).squeeze(-1).squeeze(-1)\n","\n","        # if whiten exist: pooled features -> whiten -> norm\n","        if self.whiten is not None:\n","            o = self.norm(self.whiten(o))\n","\n","        # permute so that it is Dx1 column vector per image (DxN if many images)\n","        return o.permute(1,0)"],"metadata":{"id":"S0hXL2bph3W8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torchsummary import summary\n","\n","def init_network(params):\n","\n","    # parse params with default values\n","    architecture = params.get('architecture', 'vgg16')\n","    local_whitening = params.get('local_whitening', False)\n","    pooling = params.get('pooling', 'gem')\n","    regional = params.get('regional', False)\n","    whitening = params.get('whitening', False)\n","    mean = params.get('mean', [0.485, 0.456, 0.406])\n","    std = params.get('std', [0.229, 0.224, 0.225])\n","    pretrained = params.get('pretrained', True)\n","\n","    # get output dimensionality size\n","    dim = 512\n","\n","    # loading network from torchvision\n","    net_in = getattr(torchvision.models, architecture)(pretrained=False)\n","    print(summary(net_in, (3, 224, 224)))\n","    # initialize features\n","    # take only convolutions for features,\n","    # always ends with ReLU to make last activations non-negative\n","    if architecture.startswith('alexnet'):\n","        features = list(net_in.features.children())[:-1]\n","    elif architecture.startswith('vgg'):\n","        features = list(net_in.features.children())[:-1]\n","    elif architecture.startswith('resnet'):\n","        features = list(net_in.children())[:-2]\n","    elif architecture.startswith('densenet'):\n","        features = list(net_in.features.children())\n","        features.append(nn.ReLU(inplace=True))\n","    elif architecture.startswith('squeezenet'):\n","        features = list(net_in.features.children())\n","    else:\n","        raise ValueError('Unsupported or unknown architecture: {}!'.format(architecture))\n","\n","    # initialize local whitening\n","    if local_whitening:\n","        lwhiten = nn.Linear(dim, dim, bias=True)\n","        # TODO: lwhiten with possible dimensionality reduce\n","\n","        if pretrained:\n","            lw = architecture\n","            if lw in L_WHITENING:\n","                print(\">> {}: for '{}' custom computed local whitening '{}' is used\"\n","                    .format(os.path.basename(__file__), lw, os.path.basename(L_WHITENING[lw])))\n","                whiten_dir = os.path.join(get_data_root(), 'whiten')\n","                lwhiten.load_state_dict(model_zoo.load_url(L_WHITENING[lw], model_dir=whiten_dir))\n","            else:\n","                print(\">> {}: for '{}' there is no local whitening computed, random weights are used\"\n","                    .format(os.path.basename(__file__), lw))\n","\n","    else:\n","        lwhiten = None\n","    \n","    # initialize pooling\n","    pool = GeM()\n","    # initialize regional pooling\n","    if regional:\n","        rpool = pool\n","        rwhiten = nn.Linear(dim, dim, bias=True)\n","        # TODO: rwhiten with possible dimensionality reduce\n","\n","        if pretrained:\n","            rw = '{}-{}-r'.format(architecture, pooling)\n","            if rw in R_WHITENING:\n","                print(\">> {}: for '{}' custom computed regional whitening '{}' is used\"\n","                    .format(os.path.basename(__file__), rw, os.path.basename(R_WHITENING[rw])))\n","                whiten_dir = os.path.join(get_data_root(), 'whiten')\n","                rwhiten.load_state_dict(model_zoo.load_url(R_WHITENING[rw], model_dir=whiten_dir))\n","            else:\n","                print(\">> {}: for '{}' there is no regional whitening computed, random weights are used\"\n","                    .format(os.path.basename(__file__), rw))\n","\n","        pool = Rpool(rpool, rwhiten)\n","\n","    # initialize whitening\n","    if whitening:\n","        whiten = nn.Linear(dim, dim, bias=True)\n","        # TODO: whiten with possible dimensionality reduce\n","\n","        if pretrained:\n","            w = architecture\n","            if local_whitening:\n","                w += '-lw'\n","            w += '-' + pooling\n","            if regional:\n","                w += '-r'\n","            if w in WHITENING:\n","                print(\">> {}: for '{}' custom computed whitening '{}' is used\"\n","                    .format(os.path.basename(__file__), w, os.path.basename(WHITENING[w])))\n","                whiten_dir = os.path.join(get_data_root(), 'whiten')\n","                whiten.load_state_dict(model_zoo.load_url(WHITENING[w], model_dir=whiten_dir))\n","            else:\n","                print(\">> {}: for '{}' there is no whitening computed, random weights are used\"\n","                    .format(os.path.basename(__file__), w))\n","    else:\n","        whiten = None\n","\n","    # create meta information to be stored in the network\n","    meta = {\n","        'architecture' : architecture, \n","        'local_whitening' : local_whitening, \n","        'pooling' : pooling, \n","        'regional' : regional, \n","        'whitening' : whitening, \n","        'mean' : mean, \n","        'std' : std,\n","        'outputdim' : dim,\n","    }\n","\n","    # create a generic image retrieval network\n","    net = ImageRetrievalNet(features, lwhiten, pool, whiten, meta)\n","\n","    # initialize features with custom pretrained network if needed\n","    if pretrained and architecture in FEATURES:\n","        print(\">> {}: for '{}' custom pretrained features '{}' are used\"\n","            .format(os.path.basename(__file__), architecture, os.path.basename(FEATURES[architecture])))\n","        model_dir = os.path.join(get_data_root(), 'networks')\n","        net.features.load_state_dict(model_zoo.load_url(FEATURES[architecture], model_dir=model_dir))\n","\n","    return net"],"metadata":{"id":"xDIuYocEihze"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["state = torch.load('/content/drive/MyDrive/Yonin-IR/data/retrievalSfM120k-vgg16-gem-b4dcdc6.pth')\n","\n","net_params = {}\n","net_params['architecture'] = state['meta']['architecture']\n","net_params['pooling'] = state['meta']['pooling']\n","net_params['local_whitening'] = state['meta'].get('local_whitening', False)\n","net_params['regional'] = state['meta'].get('regional', False)\n","net_params['whitening'] = state['meta'].get('whitening', False)\n","net_params['mean'] = state['meta']['mean']\n","net_params['std'] = state['meta']['std']\n","net_params['pretrained'] = False\n","\n","net = init_network(net_params)\n","net.load_state_dict(state['state_dict'])\n","##Mạng VGG16 ban đầu "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WtRLxX60UqU9","executionInfo":{"status":"ok","timestamp":1643362345890,"user_tz":-420,"elapsed":3916,"user":{"displayName":"Ngọc Phan Tiến","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01365114518883932849"}},"outputId":"9070651d-ffd3-4fb1-a431-33c1bb8f42d5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1         [-1, 64, 224, 224]           1,792\n","              ReLU-2         [-1, 64, 224, 224]               0\n","            Conv2d-3         [-1, 64, 224, 224]          36,928\n","              ReLU-4         [-1, 64, 224, 224]               0\n","         MaxPool2d-5         [-1, 64, 112, 112]               0\n","            Conv2d-6        [-1, 128, 112, 112]          73,856\n","              ReLU-7        [-1, 128, 112, 112]               0\n","            Conv2d-8        [-1, 128, 112, 112]         147,584\n","              ReLU-9        [-1, 128, 112, 112]               0\n","        MaxPool2d-10          [-1, 128, 56, 56]               0\n","           Conv2d-11          [-1, 256, 56, 56]         295,168\n","             ReLU-12          [-1, 256, 56, 56]               0\n","           Conv2d-13          [-1, 256, 56, 56]         590,080\n","             ReLU-14          [-1, 256, 56, 56]               0\n","           Conv2d-15          [-1, 256, 56, 56]         590,080\n","             ReLU-16          [-1, 256, 56, 56]               0\n","        MaxPool2d-17          [-1, 256, 28, 28]               0\n","           Conv2d-18          [-1, 512, 28, 28]       1,180,160\n","             ReLU-19          [-1, 512, 28, 28]               0\n","           Conv2d-20          [-1, 512, 28, 28]       2,359,808\n","             ReLU-21          [-1, 512, 28, 28]               0\n","           Conv2d-22          [-1, 512, 28, 28]       2,359,808\n","             ReLU-23          [-1, 512, 28, 28]               0\n","        MaxPool2d-24          [-1, 512, 14, 14]               0\n","           Conv2d-25          [-1, 512, 14, 14]       2,359,808\n","             ReLU-26          [-1, 512, 14, 14]               0\n","           Conv2d-27          [-1, 512, 14, 14]       2,359,808\n","             ReLU-28          [-1, 512, 14, 14]               0\n","           Conv2d-29          [-1, 512, 14, 14]       2,359,808\n","             ReLU-30          [-1, 512, 14, 14]               0\n","        MaxPool2d-31            [-1, 512, 7, 7]               0\n","AdaptiveAvgPool2d-32            [-1, 512, 7, 7]               0\n","           Linear-33                 [-1, 4096]     102,764,544\n","             ReLU-34                 [-1, 4096]               0\n","          Dropout-35                 [-1, 4096]               0\n","           Linear-36                 [-1, 4096]      16,781,312\n","             ReLU-37                 [-1, 4096]               0\n","          Dropout-38                 [-1, 4096]               0\n","           Linear-39                 [-1, 1000]       4,097,000\n","================================================================\n","Total params: 138,357,544\n","Trainable params: 138,357,544\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.57\n","Forward/backward pass size (MB): 218.78\n","Params size (MB): 527.79\n","Estimated Total Size (MB): 747.15\n","----------------------------------------------------------------\n","None\n"]},{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["summary(net, (3, 224, 224))\n","#Mạng VGG16 có chỉnh sửa"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZwIemv4M0_6j","executionInfo":{"status":"ok","timestamp":1643362351915,"user_tz":-420,"elapsed":1079,"user":{"displayName":"Ngọc Phan Tiến","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01365114518883932849"}},"outputId":"2eb3543c-14d6-4858-c94d-1e4ae1512c3e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1         [-1, 64, 224, 224]           1,792\n","              ReLU-2         [-1, 64, 224, 224]               0\n","            Conv2d-3         [-1, 64, 224, 224]          36,928\n","              ReLU-4         [-1, 64, 224, 224]               0\n","         MaxPool2d-5         [-1, 64, 112, 112]               0\n","            Conv2d-6        [-1, 128, 112, 112]          73,856\n","              ReLU-7        [-1, 128, 112, 112]               0\n","            Conv2d-8        [-1, 128, 112, 112]         147,584\n","              ReLU-9        [-1, 128, 112, 112]               0\n","        MaxPool2d-10          [-1, 128, 56, 56]               0\n","           Conv2d-11          [-1, 256, 56, 56]         295,168\n","             ReLU-12          [-1, 256, 56, 56]               0\n","           Conv2d-13          [-1, 256, 56, 56]         590,080\n","             ReLU-14          [-1, 256, 56, 56]               0\n","           Conv2d-15          [-1, 256, 56, 56]         590,080\n","             ReLU-16          [-1, 256, 56, 56]               0\n","        MaxPool2d-17          [-1, 256, 28, 28]               0\n","           Conv2d-18          [-1, 512, 28, 28]       1,180,160\n","             ReLU-19          [-1, 512, 28, 28]               0\n","           Conv2d-20          [-1, 512, 28, 28]       2,359,808\n","             ReLU-21          [-1, 512, 28, 28]               0\n","           Conv2d-22          [-1, 512, 28, 28]       2,359,808\n","             ReLU-23          [-1, 512, 28, 28]               0\n","        MaxPool2d-24          [-1, 512, 14, 14]               0\n","           Conv2d-25          [-1, 512, 14, 14]       2,359,808\n","             ReLU-26          [-1, 512, 14, 14]               0\n","           Conv2d-27          [-1, 512, 14, 14]       2,359,808\n","             ReLU-28          [-1, 512, 14, 14]               0\n","           Conv2d-29          [-1, 512, 14, 14]       2,359,808\n","             ReLU-30          [-1, 512, 14, 14]               0\n","              GeM-31            [-1, 512, 1, 1]               0\n","              L2N-32            [-1, 512, 1, 1]               0\n","================================================================\n","Total params: 14,714,688\n","Trainable params: 14,714,688\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.57\n","Forward/backward pass size (MB): 218.21\n","Params size (MB): 56.13\n","Estimated Total Size (MB): 274.92\n","----------------------------------------------------------------\n"]}]},{"cell_type":"markdown","source":["##Tiền xử lý ảnh đầu vào "],"metadata":{"id":"4VOnQGRKCwv7"}},{"cell_type":"code","source":["def image_preprocess(img):\n","    img = img.resize((362, 362))\n","    img = img.convert(\"RGB\")\n","    x = image.img_to_array(img)\n","    x = x.T\n","    x = np.expand_dims(x, axis=0)\n","\n","    #x = preprocess_input(x)\n","    x = torch.Tensor(x)\n","    #print(x.size())\n","    return x"],"metadata":{"id":"J1xsmaDJz0sI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Extract feature"],"metadata":{"id":"BCVl-JSUDIa7"}},{"cell_type":"code","source":["def extract_vector(model, img_path):\n","    print(\"Xu ly: \", img_path)\n","    img = Image.open(img_path)\n","    img_tensor = image_preprocess(img)\n","\n","    vector = model(img_tensor).cpu().data.squeeze()\n","\n","    return vector"],"metadata":{"id":"2lGltz3D1ZV5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#dataset_path = \"/content/oxbuild/\"\n","dataset_path = '/content/drive/MyDrive/Yonin-IR/data/query_image_paris/'\n","data = {\n","        \"paths\": [],\n","        \"names\": [],\n","        \"vectors\": []\n","}\n","\n","listdir = os.listdir(dataset_path)\n","\n","for file_paris in listdir:\n","\n","    full_path = dataset_path + file_paris\n","    \n","    try:\n","        image_vector = extract_vector(net, full_path)\n","        data[\"vectors\"].append(image_vector.tolist())\n","        data[\"paths\"].append(full_path)\n","        data['names'].append(file_paris.split('.')[0])\n","    except:\n","        print(full_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gO2sx9K7Tp5t","executionInfo":{"status":"ok","timestamp":1643373888385,"user_tz":-420,"elapsed":90052,"user":{"displayName":"Ngọc Phan Tiến","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01365114518883932849"}},"outputId":"147fad34-124c-4528-c7ae-0c0f862e69e5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Xu ly:  /content/drive/MyDrive/Yonin-IR/data/query_image_paris/defense_1.jpg\n","Xu ly:  /content/drive/MyDrive/Yonin-IR/data/query_image_paris/defense_4.jpg\n","Xu ly:  /content/drive/MyDrive/Yonin-IR/data/query_image_paris/defense_3.jpg\n","Xu ly:  /content/drive/MyDrive/Yonin-IR/data/query_image_paris/defense_5.jpg\n","Xu ly:  /content/drive/MyDrive/Yonin-IR/data/query_image_paris/defense_2.jpg\n","Xu ly:  /content/drive/MyDrive/Yonin-IR/data/query_image_paris/eiffel_4.jpg\n","Xu ly:  /content/drive/MyDrive/Yonin-IR/data/query_image_paris/eiffel_3.jpg\n","Xu ly:  /content/drive/MyDrive/Yonin-IR/data/query_image_paris/eiffel_1.jpg\n","Xu ly:  /content/drive/MyDrive/Yonin-IR/data/query_image_paris/eiffel_2.jpg\n","Xu ly:  /content/drive/MyDrive/Yonin-IR/data/query_image_paris/invalides_1.jpg\n","Xu ly:  /content/drive/MyDrive/Yonin-IR/data/query_image_paris/eiffel_5.jpg\n","Xu ly:  /content/drive/MyDrive/Yonin-IR/data/query_image_paris/invalides_4.jpg\n","Xu ly:  /content/drive/MyDrive/Yonin-IR/data/query_image_paris/invalides_3.jpg\n","Xu ly:  /content/drive/MyDrive/Yonin-IR/data/query_image_paris/invalides_2.jpg\n","Xu ly:  /content/drive/MyDrive/Yonin-IR/data/query_image_paris/invalides_5.jpg\n","Xu ly:  /content/drive/MyDrive/Yonin-IR/data/query_image_paris/louvre_1.jpg\n","Xu ly:  /content/drive/MyDrive/Yonin-IR/data/query_image_paris/louvre_2.jpg\n","Xu ly:  /content/drive/MyDrive/Yonin-IR/data/query_image_paris/louvre_5.jpg\n","Xu ly:  /content/drive/MyDrive/Yonin-IR/data/query_image_paris/louvre_4.jpg\n","Xu ly:  /content/drive/MyDrive/Yonin-IR/data/query_image_paris/louvre_3.jpg\n","Xu ly:  /content/drive/MyDrive/Yonin-IR/data/query_image_paris/moulinrouge_1.jpg\n","Xu ly:  /content/drive/MyDrive/Yonin-IR/data/query_image_paris/moulinrouge_4.jpg\n","Xu ly:  /content/drive/MyDrive/Yonin-IR/data/query_image_paris/moulinrouge_2.jpg\n","Xu ly:  /content/drive/MyDrive/Yonin-IR/data/query_image_paris/museedorsay_1.jpg\n","Xu ly:  /content/drive/MyDrive/Yonin-IR/data/query_image_paris/moulinrouge_3.jpg\n","Xu ly:  /content/drive/MyDrive/Yonin-IR/data/query_image_paris/moulinrouge_5.jpg\n","Xu ly:  /content/drive/MyDrive/Yonin-IR/data/query_image_paris/museedorsay_3.jpg\n","Xu ly:  /content/drive/MyDrive/Yonin-IR/data/query_image_paris/notredame_1.jpg\n","Xu ly:  /content/drive/MyDrive/Yonin-IR/data/query_image_paris/museedorsay_4.jpg\n","Xu ly:  /content/drive/MyDrive/Yonin-IR/data/query_image_paris/museedorsay_5.jpg\n","Xu ly:  /content/drive/MyDrive/Yonin-IR/data/query_image_paris/museedorsay_2.jpg\n","Xu ly:  /content/drive/MyDrive/Yonin-IR/data/query_image_paris/pantheon_1.jpg\n","Xu ly:  /content/drive/MyDrive/Yonin-IR/data/query_image_paris/notredame_3.jpg\n","Xu ly:  /content/drive/MyDrive/Yonin-IR/data/query_image_paris/notredame_4.jpg\n","Xu ly:  /content/drive/MyDrive/Yonin-IR/data/query_image_paris/notredame_5.jpg\n","Xu ly:  /content/drive/MyDrive/Yonin-IR/data/query_image_paris/notredame_2.jpg\n","Xu ly:  /content/drive/MyDrive/Yonin-IR/data/query_image_paris/pantheon_2.jpg\n","Xu ly:  /content/drive/MyDrive/Yonin-IR/data/query_image_paris/pompidou_1.jpg\n","Xu ly:  /content/drive/MyDrive/Yonin-IR/data/query_image_paris/pantheon_3.jpg\n","Xu ly:  /content/drive/MyDrive/Yonin-IR/data/query_image_paris/pantheon_4.jpg\n","Xu ly:  /content/drive/MyDrive/Yonin-IR/data/query_image_paris/pompidou_4.jpg\n","Xu ly:  /content/drive/MyDrive/Yonin-IR/data/query_image_paris/pompidou_3.jpg\n","Xu ly:  /content/drive/MyDrive/Yonin-IR/data/query_image_paris/pantheon_5.jpg\n","Xu ly:  /content/drive/MyDrive/Yonin-IR/data/query_image_paris/pompidou_2.jpg\n","Xu ly:  /content/drive/MyDrive/Yonin-IR/data/query_image_paris/sacrecoeur_1.jpg\n","Xu ly:  /content/drive/MyDrive/Yonin-IR/data/query_image_paris/sacrecoeur_4.jpg\n","Xu ly:  /content/drive/MyDrive/Yonin-IR/data/query_image_paris/sacrecoeur_3.jpg\n","Xu ly:  /content/drive/MyDrive/Yonin-IR/data/query_image_paris/pompidou_5.jpg\n","Xu ly:  /content/drive/MyDrive/Yonin-IR/data/query_image_paris/sacrecoeur_2.jpg\n","Xu ly:  /content/drive/MyDrive/Yonin-IR/data/query_image_paris/triomphe_2.jpg\n","Xu ly:  /content/drive/MyDrive/Yonin-IR/data/query_image_paris/triomphe_5.jpg\n","Xu ly:  /content/drive/MyDrive/Yonin-IR/data/query_image_paris/triomphe_3.jpg\n","Xu ly:  /content/drive/MyDrive/Yonin-IR/data/query_image_paris/triomphe_1.jpg\n","Xu ly:  /content/drive/MyDrive/Yonin-IR/data/query_image_paris/triomphe_4.jpg\n","Xu ly:  /content/drive/MyDrive/Yonin-IR/data/query_image_paris/sacrecoeur_5.jpg\n"]}]},{"cell_type":"code","source":["len(data['vectors'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0pCO6VZz3q9W","executionInfo":{"status":"ok","timestamp":1643129127101,"user_tz":-420,"elapsed":20,"user":{"displayName":"Ngọc Phan Tiến","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01365114518883932849"}},"outputId":"537b2c14-830e-493e-a460-b99dca701408"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["55"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["##Lưu file extract "],"metadata":{"id":"GHr6uHrODNMu"}},{"cell_type":"code","source":["json_path = \"/content/drive/MyDrive/Yonin-IR/data/data_paris_vgg16_pretrain_query_362.json\"\n","with open(json_path, \"w\") as fp:\n","    json.dump(data, fp, indent=4)"],"metadata":{"id":"XRcZ8Mwz5izc"},"execution_count":null,"outputs":[]}]}